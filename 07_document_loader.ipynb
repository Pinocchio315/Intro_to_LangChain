{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm_model = 'gpt-4o-mini'\n",
    "chat_model = ChatOpenAI(temperature=0.7, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "### pip install pypdf\n",
    "\n",
    "loader = PyPDFLoader(\"./data/react-paper.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 페이지 수:  33\n",
      "1페이지의 첫 300 글자:  Published as a conference paper at ICLR 2023\n",
      "REAC T: S YNERGIZING REASONING AND ACTING IN\n",
      "LANGUAGE MODELS\n",
      "Shunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\n",
      "1Department of Computer Science, Princeton University\n",
      "2Google Research, Brain team\n",
      "1{shunyuy,kart\n",
      "1페이지의 메타데이터:  {'source': './data/react-paper.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"총 페이지 수: \", len(pages))\n",
    "\n",
    "page = pages[0] # 첫번째 페이지\n",
    "\n",
    "print(\"1페이지의 첫 300 글자: \", page.page_content[0:300]) # first 700 characters on the page\n",
    "print(\"1페이지의 메타데이터: \", page.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쪼개진 Chunk의 개수:  3\n",
      "두번째 Chunk의 내용:  Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "세번째 Chunk의 내용:  Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 1. CharacterTextSplitter\n",
    "with open(\"./data/example.txt\") as paper:\n",
    "    speech = paper.read()\n",
    "    \n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([speech])\n",
    "print(\"쪼개진 Chunk의 개수: \", len(texts))\n",
    "print(\"두번째 Chunk의 내용: \", texts[1].page_content)\n",
    "print()\n",
    "print(\"세번째 Chunk의 내용: \", texts[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "Doc 1: ﻿As far as black Americans were concerned, the nation’s response to Brown was agonizingly slow, and neither state\n",
      "Doc 2: slow, and neither state legislatures nor the Congress seemed willing to help their cause along. Finally, President John\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 2. RecursiveCharacterTextSplitter\n",
    "with open(\"./data/i-have-a-dream.txt\") as paper:\n",
    "    speech = paper.read()\n",
    "    \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 120,\n",
    "    chunk_overlap = 30,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents([speech])\n",
    "\n",
    "print(len(docs))\n",
    "print(f\"Doc 1: {docs[0].page_content}\")\n",
    "print(f\"Doc 2: {docs[1].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcdefghijkl', 'hijklmnopqrs', 'opqrstuvwxyz']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "s = \"Python can be easy to pick up whether you're a professional or a beginner.\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 12,\n",
    "    chunk_overlap = 5,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "text = text_splitter.split_text(s1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
