{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm_model = 'gpt-4o-mini'\n",
    "chat_model = ChatOpenAI(temperature=0.7, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed! == 512\n",
      "Similary of text1 and text2: 19.71148677625692%\n",
      "Similary of text1 and text3: 48.524775267896196%\n",
      "[0.00929352268576622, -0.0662163496017456, -0.08923234045505524] [0.006161571945995092, -0.043901197612285614, -0.05916071683168411]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=512)\n",
    "embeddings_2 = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# Embeddings - simpler example - compare similarity etc.\n",
    "text1 = \"I love a cat.\"\n",
    "text2 = \"This is a rock.\"\n",
    "text3 = \"I like the dog.\"\n",
    "\n",
    "embed1 = embeddings.embed_query(text1)\n",
    "embed12 = embeddings_2.embed_query(text1)\n",
    "embed2 = embeddings.embed_query(text2)\n",
    "embed3 = embeddings.embed_query(text3)\n",
    "print(f\"Embed! == {len(embed1)}\")\n",
    "import numpy as np\n",
    "similarity_13 = np.dot(embed1, embed3)\n",
    "similarity_12 = np.dot(embed1, embed2)\n",
    "print(f\"Similary of text1 and text2: {similarity_12*100}%\")\n",
    "print(f\"Similary of text1 and text3: {similarity_13*100}%\")\n",
    "\n",
    "\n",
    "print(embed1[:3], embed12[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19711487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity([embed1], [embed2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쪼개진 Chunk의 개수:  33\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"./data/react-paper.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(pages)\n",
    "print(\"쪼개진 Chunk의 개수: \", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "persist_directory = \"./data/db/chroma\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings, # openai embeddings\n",
    "    persist_directory=persist_directory\n",
    "    )\n",
    "print(vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 4, 'source': './data/react-paper.pdf'}, page_content='Published as a conference paper at ICLR 2023\\nPrompt Methoda HotpotQA Fever\\n(EM) (Acc)\\nStandard 28.7 57.1\\nCoT(Wei et al., 2022) 29.4 56.3\\nCoT-SC(Wang et al., 2022a) 33.4 60.4\\nAct 25.7 58.9\\nReAct 27.4 60.9\\nCoT-SC→ReAct 34.2 64.6\\nReAct→CoT-SC 35.1 62.0\\nSupervised SoTAb 67.5 89.5\\nTable 1: PaLM-540B prompting results on\\nHotpotQA and Fever.\\naHotpotQA EM is 27.1, 28.9, 33.8 for Standard, CoT,\\nCoT-SC in Wang et al. (2022b).\\nb(Zhu et al., 2021; Lewis et al., 2020)\\n0 5 10 15 20\\n#CoT-SC trials\\n26\\n28\\n30\\n32\\n34HotpotQA EM\\n0 5 10 15 20\\n#CoT-SC trials\\n47.5\\n50.0\\n52.5\\n55.0\\n57.5\\n60.0\\n62.5\\n65.0Fever Acc\\nMethod\\nCoT-SC -> ReAct\\nReAct -> CoT-SC\\nCoT-SC\\nReAct\\nCoT\\nFigure 2: PaLM-540B prompting results with respect to\\nnumber of CoT-SC samples used.\\nsearch reformulation (“maybe I can search/look up x instead”), and synthesize the ﬁnal answer (“...so\\nthe answer is x”). See Appendix C for more details.\\nBaselines We systematically ablateReAct trajectories to build prompts for multiple baselines (with\\nformats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,\\nactions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,\\n2022), which removes actions and observations and serve as a reasoning-only baseline. We also\\nbuild a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories\\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\\nconsistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts\\nin ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the\\nInternet to answer questions, though it operates on a different task and action space, and uses imitation\\nand reinforcement learning instead of prompting.\\nCombining Internal and External Knowledge As will be detail in Section 3.3, we observe that\\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\\nor thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide\\nwhen to switch to the other method based on the following heuristics: A) ReAct →CoT-SC: when\\nReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for\\nHotpotQA and FEVER respectively as we ﬁnd more steps will not improve ReAct performance3.\\nB) CoT-SC →ReAct: when the majority answer among nCoT-SC samples occurs less than n/2\\ntimes (i.e. internal knowledge might not support the task conﬁdently), back off to ReAct.\\nFinetuning Due to the challenge of manually annotating reasoning traces and actions at scale,\\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\\nwith correct answers generated by ReAct (also for other baselines) to ﬁnetune smaller language\\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\\ninput questions/claims. More details are in Appendix B.1.\\n3.3 R ESULTS AND OBSERVATIONS\\nReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-\\n540B as the base model with different prompting methods. We note that ReAct is better than Act\\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\\nﬁnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conﬁrm the beneﬁt of reasoning\\ntraces for more informed acting.\\n3Of all trajectories with correct ﬁnal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only\\ntake up 0.84% and 1.33% respectively.\\n5'), Document(metadata={'page': 7, 'source': './data/react-paper.pdf'}, page_content='Published as a conference paper at ICLR 2023\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nAct(best of 6) 88 42 74 67 72 41 45\\nReAct(avg) 65 39 83 76 55 24 57\\nReAct(best of 6) 92 58 96 86 78 41 71\\nReAct-IM(avg) 55 59 60 55 23 24 48\\nReAct-IM(best of 6) 62 68 87 57 39 33 53\\nBUTLERg (best of 8) 33 26 70 76 17 12 22\\nBUTLER(best of 8) 46 39 74 100 22 24 37\\nTable 3: AlfWorld task-speciﬁc success rates (%). BUTLER and\\nBUTLERg results are from Table 4 of Shridhar et al. (2020b). All\\nmethods use greedy decoding, except that BUTLER uses beam search.\\nMethod Score SR\\nAct 62.3 30.1\\nReAct 66.6 40.0\\nIL 59.9 29.1\\nIL+RL 62.4 28.7\\nHuman 82.1 59.6Expert\\nTable 4: Score and suc-\\ncess rate (SR) on Web-\\nshop. IL/IL+RL taken\\nfrom Yao et al. (2022).\\ntrained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)\\nmethod additionally trained with 10,587 training instructions.\\nResults ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On\\nALFWorld, the bestReAct trial achieves an average success rate of 71%, signiﬁcantly outperforming\\nthe best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats\\nthe best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across\\nsix controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.\\nQualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals\\ninto smaller subgoals, or loses track of the current state of the environment. Example trajectories\\ncomparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.\\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With\\nadditional sparse reasoning, ReAct achieves signiﬁcantly better performance, with an absolute 10%\\nimprovement over the previous best success rate. By checking examples, we ﬁnd that ReAct is more\\nlikely to identify instruction-relevant products and options by reasoning to bridge the gap between\\nnoisy observations and actions (e.g. “For ‘space-saving ottoman bench for living room’, the item\\nhas options ‘39x18x18inch’ and ‘blue’ and seems good to buy.”). However, existing methods are\\nstill far from the performance of expert humans (Table 4), who perform signiﬁcantly more product\\nexplorations and query re-formulations that are still challenging for prompting-based methods.\\nOn the value of internal reasoning vs. external feedback To our knowledge, ReAct is the ﬁrst\\ndemonstration of combined reasoning and action using an LLM applied to an interactive environment\\nwithin a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang\\net al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner\\nmonologue”. However, IM’s “inner monologue” is limited to observations of the environment\\nstate and what needs to be completed by the agent for the goal to be satisﬁed. In contrast, the\\nreasoning traces in ReAct for decision making is ﬂexible and sparse, allowing diverse reasoning\\ntypes (see Section 2) to be induced for different tasks.\\nTo demonstrate the differences between ReAct and IM, and to highlight the importance of internal\\nreasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought\\npattern composed of IM-like dense external feedback. As can be seen in Table 3,ReAct substantially\\noutperforms IM-style prompting ( ReAct-IM) (71 vs. 53 overall success rate), with consistent\\nadvantages on ﬁve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes\\nin identifying when subgoals were ﬁnished, or what the next subgoal should be, due to a lack of high-\\nlevel goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where\\nan item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.\\nBoth shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in\\nAppendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example\\ntrajectory in Appendix D.2.3.\\n8'), Document(metadata={'page': 5, 'source': './data/react-paper.pdf'}, page_content='Published as a conference paper at ICLR 2023\\nType Deﬁnition ReAct CoT\\nSuccess True positive Correct reasoning trace and facts 94% 86%\\nFalse positive Hallucinated reasoning trace or facts 6% 14%\\nFailure\\nReasoning error Wrong reasoning trace (including failing to recover from repetitive steps) 47% 16%\\nSearch result error Search return empty or does not contain useful information 23% -\\nHallucination Hallucinated reasoning trace or facts 0% 56%\\nLabel ambiguity Right prediction but did not match the label precisely 29% 28%\\nTable 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their\\npercentages in randomly selected examples studied by human.\\nReAct vs. CoT On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly\\nlags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\\ndiffer by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge\\nis vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we\\nrandomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\\nand CoT respectively (thus 200 examples in total), and manually labeled their success and failure\\nmodes in Table 2. Some key observations are as follows:\\nA) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than\\nReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the\\nproblem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the\\naccess of an external knowledge base.\\nB) While interleaving reasoning, action and observation steps improves ReAct’s grounded-\\nness and trustworthiness, such a structural constraint also reduces its ﬂexibility in formulating\\nreasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent\\nerror pattern speciﬁc to ReAct, in which the model repetitively generates the previous thoughts and\\nactions, and we categorize it as part of “reasoning error” as the model fails to reason about what the\\nproper next action to take and jump out of the loop4.\\nC) For ReAct, successfully retrieving informative knowledge via search is critical. Non-\\ninformative search, which counts for 23% of the error cases, derails the model reasoning and gives\\nit a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between\\nfactuality and ﬂexibility, which motivates our proposed strategies of combining two methods.\\nWe provide examples for each success and failure modes in Appendix E.1. We also ﬁnd some\\nHotpotQA questions may contain outdated answer labels, see Figure 4 for example.\\nReAct + CoT-SC perform best for prompting LLMs Also shown in Table 1, the best prompting\\nmethod on HotpotQA and Fever are ReAct →CoT-SC and CoT-SC →ReAct respectively.\\nFurthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC\\nsamples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both\\nsigniﬁcantly and consistently outperform CoT-SC across different number of samples, reaching\\nCoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of\\nproperly combining model internal knowledge and external knowledge for reasoning tasks.\\nReAct performs best for ﬁne-tuning Figure 3 shows the scaling effect of prompting/ﬁnetuning\\nfour methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, promptingReAct\\nperforms worst among four methods due to the difﬁculty to learn both reasoning and acting from\\nin-context examples. However, when ﬁnetuned with just 3,000 examples, ReAct becomes the best\\nmethod among the four, with PaLM-8B ﬁnetuned ReAct outperforming all PaLM-62B prompting\\nmethods, and PaLM-62B ﬁnetuned ReAct outperforming all 540B prompting methods. In contrast,\\nﬁnetuning Standard or CoT is signiﬁcantly worse than ﬁnetuning ReAct or Act for both PaLM-\\n8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge\\nfacts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a\\nmore generalizable skill for knowledge reasoning. As all prompting methods are still signiﬁcantly\\nfar from domain-speciﬁc state-of-the-art approaches (Table 1), we believe ﬁnetuning with more\\nhuman-written data might be a better way to unleash the power of ReAct.\\n4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using\\nbetter decoding (e.g. beam search) might help address this issue.\\n6'), Document(metadata={'page': 3, 'source': './data/react-paper.pdf'}, page_content='Published as a conference paper at ICLR 2023\\nappear sparsely in the most relevant positions of a trajectory, so we let the language model decide the\\nasynchronous occurrence of thoughts and actions for itself.\\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct\\nenjoys several unique features: A) Intuitive and easy to design : Designing ReAct prompts is\\nstraightforward as human annotators just type down their thoughts in language on top of their actions\\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\\nprompt design for each task in Sections 3 and 4. B) General and ﬂexible: Due to the ﬂexible thought\\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\\nspaces and reasoning needs, including but not limited to QA, fact veriﬁcation, text game, and web\\nnavigation. C) Performant and robust: ReAct shows strong generalization to new task instances\\nwhile learning solely from one to six in-context examples, consistently outperforming baselines with\\nonly reasoning or acting across different domains. We also show in Section 3 additional beneﬁts\\nwhen ﬁnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\\nD) Human aligned and controllable: ReAct promises an interpretable sequential decision making\\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\\nFigure 5 in Section 4.\\n3 K NOWLEDGE -INTENSIVE REASONING TASKS\\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact\\nveriﬁcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to\\nretrieve information to support reasoning, while also use reasoning to target what to retrieve next,\\ndemonstrating a synergy of reasoning and acting.\\n3.1 S ETUP\\nDomains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\\nPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\\nover two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriﬁcation\\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\\non if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\\nsetup for both tasks, where models only receive the question/claim as input without access to support\\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\\nan external environment to support reasoning.\\nAction Space We design a simple Wikipedia web API with three types of actions to support\\ninteractive information retrieval: (1) search[entity], which returns the ﬁrst 5 sentences from\\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\\nWikipedia search engine, (2) lookup[string], which would return the next sentence in the page\\ncontaining string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which\\nwould ﬁnish the current task with answer. We note that this action space mostly can only retrieve a\\nsmall part of a passage based on exact passage name, which is signiﬁcantly weaker than state-of-the-\\nart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\\nand force models to retrieve via explicit reasoning in language.\\n3.2 M ETHODS\\nReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training\\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\\nthought), where free-form thoughts are used for various purposes. Speciﬁcally, we use a combination\\nof thoughts that decompose questions (“I need to search x, ﬁnd y, then ﬁnd z”), extract information\\nfrom Wikipedia observations (“x was started in 1844”, “The paragraph does not tell x”), perform\\ncommonsense (“x is not y, so z must instead be...”) or arithmetic reasoning (“1844 < 1989”), guide\\n2We ﬁnd more examples do not improve performance.\\n4')]\n"
     ]
    }
   ],
   "source": [
    "query = \"what do they say about ReAct prompting method?\"\n",
    "print(vectorstore.similarity_search(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d2/b7bnbh8d1mx5rjyb450xlt200000gn/T/ipykernel_36436/1833132863.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"Tell me more about ReAct prompting\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2023\n",
      "Method Pick Clean Heat Cool Look Pick 2 All\n",
      "Act(best of 6) 88 42 74 67 72 41 45\n",
      "ReAct(avg) 65 39 83 76 55 24 57\n",
      "ReAct(best of 6) 92 58 96 86 78 41 71\n",
      "ReAct-IM(avg) 55 59 60 55 23 24 48\n",
      "ReAct-IM(best of 6) 62 68 87 57 39 33 53\n",
      "BUTLERg (best of 8) 33 26 70 76 17 12 22\n",
      "BUTLER(best of 8) 46 39 74 100 22 24 37\n",
      "Table 3: AlfWorld task-speciﬁc success rates (%). BUTLER and\n",
      "BUTLERg results are from Table 4 of Shridhar et al. (2020b). All\n",
      "methods use greedy decoding, except that BUTLER uses beam search.\n",
      "Method Score SR\n",
      "Act 62.3 30.1\n",
      "ReAct 66.6 40.0\n",
      "IL 59.9 29.1\n",
      "IL+RL 62.4 28.7\n",
      "Human 82.1 59.6Expert\n",
      "Table 4: Score and suc-\n",
      "cess rate (SR) on Web-\n",
      "shop. IL/IL+RL taken\n",
      "from Yao et al. (2022).\n",
      "trained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)\n",
      "method additionally trained with 10,587 training instructions.\n",
      "Results ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On\n",
      "ALFWorld, the bestReAct trial achieves an average success rate of 71%, signiﬁcantly outperforming\n",
      "the best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats\n",
      "the best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across\n",
      "six controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.\n",
      "Qualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals\n",
      "into smaller subgoals, or loses track of the current state of the environment. Example trajectories\n",
      "comparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.\n",
      "On Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With\n",
      "additional sparse reasoning, ReAct achieves signiﬁcantly better performance, with an absolute 10%\n",
      "improvement over the previous best success rate. By checking examples, we ﬁnd that ReAct is more\n",
      "likely to identify instruction-relevant products and options by reasoning to bridge the gap between\n",
      "noisy observations and actions (e.g. “For ‘space-saving ottoman bench for living room’, the item\n",
      "has options ‘39x18x18inch’ and ‘blue’ and seems good to buy.”). However, existing methods are\n",
      "still far from the performance of expert humans (Table 4), who perform signiﬁcantly more product\n",
      "explorations and query re-formulations that are still challenging for prompting-based methods.\n",
      "On the value of internal reasoning vs. external feedback To our knowledge, ReAct is the ﬁrst\n",
      "demonstration of combined reasoning and action using an LLM applied to an interactive environment\n",
      "within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang\n",
      "et al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner\n",
      "monologue”. However, IM’s “inner monologue” is limited to observations of the environment\n",
      "state and what needs to be completed by the agent for the goal to be satisﬁed. In contrast, the\n",
      "reasoning traces in ReAct for decision making is ﬂexible and sparse, allowing diverse reasoning\n",
      "types (see Section 2) to be induced for different tasks.\n",
      "To demonstrate the differences between ReAct and IM, and to highlight the importance of internal\n",
      "reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought\n",
      "pattern composed of IM-like dense external feedback. As can be seen in Table 3,ReAct substantially\n",
      "outperforms IM-style prompting ( ReAct-IM) (71 vs. 53 overall success rate), with consistent\n",
      "advantages on ﬁve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes\n",
      "in identifying when subgoals were ﬁnished, or what the next subgoal should be, due to a lack of high-\n",
      "level goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where\n",
      "an item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.\n",
      "Both shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in\n",
      "Appendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example\n",
      "trajectory in Appendix D.2.3.\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "docs = retriever.get_relevant_documents(\"Tell me more about ReAct prompting\")\n",
    "# print(retriever.search_type)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ReAct prompting, presented in a 2023 ICLR paper, integrates reasoning and action within large language models (LLMs) to enhance decision-making in interactive environments. It significantly outperforms traditional methods like Act and IM by allowing the model to generate thoughts alongside actions, fostering better goal decomposition and commonsense reasoning. This approach has shown improved performance in tasks such as ALFWorld and WebShop, demonstrating its effectiveness in complex, knowledge-intensive scenarios.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "        You are an assistant for question-answering tasks. \n",
    "        Use the following pieces of retrieved context to answer the question. \n",
    "        If you don't know the answer, just say that you don't know. \n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "        Question: {question} \n",
    "        Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vectorstore.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "qa_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf ./data/db/chroma\n",
    "# Chroma 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
